# Create table of 'cut'
colour_table <- data_cleaned %>% count(colour)
colour_table
# Create a map for clarity
clarity_map <- c('0' = 'IF',
'1' = 'VVS1',
'2' = 'VVS2',
'3' = 'VS1',
'4' = 'VS2',
'5' = 'SI1',
'6' = 'SI2',
'7' = 'I1')
# Create 'clarity' column based on the third character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(clarity = clarity_map[substr(c.grade, 3, 3)])
# Create table of 'cut'
clarity_table <- data_cleaned %>% count(clarity)
clarity_table
data_2 <- data_cleaned %>%
dplyr::select(cut, price, volume)
data_2 %>% head()
data_2 <- data_2 %>%
mutate(cut = as.factor(cut))
data_2 %>% head()
# Preprocessor
recipe_cp <- recipe(cut ~ price, data = data_2) %>%
step_normalize()
# Logistic regression model
log_reg_model <- logistic_reg() %>%
set_mode('classification') %>%
set_engine('glm')
# Workflow
wf_cp <- workflow() %>%
add_recipe(recipe_cp) %>%
add_model(log_reg_model)
wf_cp
# Preprocessor
recipe_cv <- recipe(cut ~ volume, data = data_2) %>%
step_normalize(all_predictors())
# Workflow
wf_cv <- workflow() %>%
add_recipe(recipe_cv) %>%
add_model(log_reg_model)
wf_cv
# Fit the models
fit_cp <- fit(wf_cp, data = data_2)
fit_cv <- fit(wf_cv, data = data_2)
# Predict probabilities and add true class to predictions
pred_price <- predict(fit_cp, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
pred_volume <- predict(fit_cv, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
# Calculate ROC curve metrics
roc_price <- pred_price %>%
roc_curve(truth = cut, .pred_ideal)
roc_volume <- pred_volume %>%
roc_curve(truth = cut, .pred_ideal)
# Add model names to each table
roc_price <- roc_price %>%
mutate(model = "Price")
roc_volume <- roc_volume %>%
mutate(model = "Volume")
# Combine the two tables
roc_data <- bind_rows(roc_price, roc_volume)
# Plot ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(x = "1 - specificity",
y = "sensitivity") +
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA, size = 0.5))
---
title: "STATS 7022 - Data Science PG Assignment 2"
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,
message=FALSE,
warning = FALSE,
fig.dim=c(6,5),
fig.align='center')
# Chunk 2
pacman::p_load(tidyverse, bookdown, readr, dplyr, tidymodels, pROC)
# Chunk 3
# Read in the data
data <- readRDS('./diamonds.rds')
# Display the first 10 lines of the data
head(data, 10)
# Chunk 4
# Check missing values
sum(is.na(data))
# Remove missing values
data_cleaned <- na.omit(data)
# Check missing values after removing
sum(is.na(data_cleaned))
# Chunk 5
# Create 'cut' column based on the first character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(
cut = ifelse(substr(c.grade, 1, 1) == '1', 'premium', 'ideal')
)
# Create table of 'cut'
cut_table <- data_cleaned %>% count(cut)
cut_table
# Chunk 6
# Create 'colour' column based on the second character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(
colour = substr(c.grade, 2, 2)
)
# Create table of 'cut'
colour_table <- data_cleaned %>% count(colour)
colour_table
# Chunk 7
# Create a map for clarity
clarity_map <- c('0' = 'IF',
'1' = 'VVS1',
'2' = 'VVS2',
'3' = 'VS1',
'4' = 'VS2',
'5' = 'SI1',
'6' = 'SI2',
'7' = 'I1')
# Create 'clarity' column based on the third character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(clarity = clarity_map[substr(c.grade, 3, 3)])
# Create table of 'cut'
clarity_table <- data_cleaned %>% count(clarity)
clarity_table
# Chunk 8
data_2 <- data_cleaned %>%
dplyr::select(cut, price, volume)
data_2 %>% head()
# Chunk 9
data_2 <- data_2 %>%
mutate(cut = as.factor(cut))
data_2 %>% head()
# Chunk 10
# Preprocessor
recipe_cp <- recipe(cut ~ price, data = data_2) %>%
step_normalize()
# Logistic regression model
log_reg_model <- logistic_reg() %>%
set_mode('classification') %>%
set_engine('glm')
# Workflow
wf_cp <- workflow() %>%
add_recipe(recipe_cp) %>%
add_model(log_reg_model)
wf_cp
# Chunk 11
# Preprocessor
recipe_cv <- recipe(cut ~ volume, data = data_2) %>%
step_normalize(all_predictors())
# Workflow
wf_cv <- workflow() %>%
add_recipe(recipe_cv) %>%
add_model(log_reg_model)
wf_cv
# Chunk 12: data_2
# Fit the models
fit_cp <- fit(wf_cp, data = data_2)
fit_cv <- fit(wf_cv, data = data_2)
# Predict probabilities and add true class to predictions
pred_price <- predict(fit_cp, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
pred_volume <- predict(fit_cv, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
# Calculate ROC curve metrics
roc_price <- pred_price %>%
roc_curve(truth = cut, .pred_ideal)
roc_volume <- pred_volume %>%
roc_curve(truth = cut, .pred_ideal)
# Add model names to each table
roc_price <- roc_price %>%
mutate(model = "Price")
roc_volume <- roc_volume %>%
mutate(model = "Volume")
# Combine the two tables
roc_data <- bind_rows(roc_price, roc_volume)
# Plot ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(x = "1 - specificity",
y = "sensitivity") +
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA, size = 0.5))
# Chunk 10
# Preprocessor
recipe_cp <- recipe(cut ~ price, data = data_2) %>%
step_normalize()
# Logistic regression model
log_reg_model <- logistic_reg() %>%
set_mode('classification') %>%
set_engine('glm')
# Workflow
wf_cp <- workflow() %>%
add_recipe(recipe_cp) %>%
add_model(log_reg_model)
wf_cp
# Chunk 11
# Preprocessor
recipe_cv <- recipe(cut ~ volume, data = data_2) %>%
step_normalize()
# Workflow
wf_cv <- workflow() %>%
add_recipe(recipe_cv) %>%
add_model(log_reg_model)
wf_cv
# Chunk 12: data_2
# Fit the models
fit_cp <- fit(wf_cp, data = data_2)
fit_cv <- fit(wf_cv, data = data_2)
# Predict probabilities and add true class to predictions
pred_price <- predict(fit_cp, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
pred_volume <- predict(fit_cv, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
# Calculate ROC curve metrics
roc_price <- pred_price %>%
roc_curve(truth = cut, .pred_ideal)
roc_volume <- pred_volume %>%
roc_curve(truth = cut, .pred_ideal)
# Add model names to each table
roc_price <- roc_price %>%
mutate(model = "Price")
roc_volume <- roc_volume %>%
mutate(model = "Volume")
# Combine the two tables
roc_data <- bind_rows(roc_price, roc_volume)
# Plot ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(x = "1 - specificity",
y = "sensitivity") +
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA, size = 0.5))
cut_table
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,
message=FALSE,
warning = FALSE,
fig.dim=c(6,5),
fig.align='center')
# Chunk 2
pacman::p_load(tidyverse, bookdown, readr, dplyr, tidymodels, pROC)
# Chunk 3
# Read in the data
data <- readRDS('./diamonds.rds')
# Display the first 10 lines of the data
data %>% head(10)
# Chunk 4
# Check missing values
sum(is.na(data))
# Remove missing values
data_cleaned <- na.omit(data)
# Check missing values after removing
sum(is.na(data_cleaned))
# Chunk 5
# Create 'cut' column based on the first character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(
cut = ifelse(substr(c.grade, 1, 1) == '1', 'premium', 'ideal')
)
# Create table of 'cut'
cut_table <- data_cleaned %>% count(cut)
# Display the table
cut_table
# Chunk 6
# Create 'colour' column based on the second character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(
colour = substr(c.grade, 2, 2)
)
# Create table of 'cut'
colour_table <- data_cleaned %>% count(colour)
# Display the table
colour_table
# Chunk 7
# Create a map for clarity
clarity_map <- c('0' = 'IF',
'1' = 'VVS1',
'2' = 'VVS2',
'3' = 'VS1',
'4' = 'VS2',
'5' = 'SI1',
'6' = 'SI2',
'7' = 'I1')
# Create 'clarity' column based on the third character of 'c.grade'
data_cleaned <- data_cleaned %>%
mutate(clarity = clarity_map[substr(c.grade, 3, 3)])
# Create table of 'cut'
clarity_table <- data_cleaned %>% count(clarity)
# Display the table
clarity_table
# Chunk 8
# Select variables
data_2 <- data_cleaned %>%
dplyr::select(cut, price, volume)
# Display the first 10 lines of the data
data_2 %>% head(10)
# Chunk 9
# Convert 'cut' to factor
data_2 <- data_2 %>%
mutate(cut = as.factor(cut))
# Display the first 10 lines of the data
data_2 %>% head(10)
# Chunk 10
# Preprocessor
recipe_cp <- recipe(cut ~ price, data = data_2) %>%
step_normalize()
# Logistic regression model
log_reg_model <- logistic_reg() %>%
set_mode('classification') %>%
set_engine('glm')
# Workflow
wf_cp <- workflow() %>%
add_recipe(recipe_cp) %>%
add_model(log_reg_model)
wf_cp
# Chunk 11
# Preprocessor
recipe_cv <- recipe(cut ~ volume, data = data_2) %>%
step_normalize()
# Workflow
wf_cv <- workflow() %>%
add_recipe(recipe_cv) %>%
add_model(log_reg_model)
wf_cv
# Chunk 12: data_2
# Fit the models
fit_cp <- fit(wf_cp, data = data_2)
fit_cv <- fit(wf_cv, data = data_2)
# Predict probabilities and add true class to predictions
pred_price <- predict(fit_cp, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
pred_volume <- predict(fit_cv, data_2, type = "prob") %>%
bind_cols(data_2 %>% dplyr::select(cut))
# Calculate ROC curve metrics
roc_price <- pred_price %>%
roc_curve(truth = cut, .pred_ideal)
roc_volume <- pred_volume %>%
roc_curve(truth = cut, .pred_ideal)
# Add model names to each table
roc_price <- roc_price %>%
mutate(model = "Price")
roc_volume <- roc_volume %>%
mutate(model = "Volume")
# Combine the two tables
roc_data <- bind_rows(roc_price, roc_volume)
# Plot ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(x = "1 - specificity",
y = "sensitivity") +
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA, size = 0.5))
# Display the table
print(cut_table)
# Display the table
data.frame(cut_table)
# Display the table
table(cut_table)
# Display the table
cut_table
get_QDA <- function(x, y, x0) {
# Load necessary libraries
if (!require(tidyverse)) install.packages("tidyverse", dependencies=TRUE)
library(tidyverse)
if (!require(dplyr)) install.packages("dplyr", dependencies=TRUE)
library(dplyr)
# Check input types
if (!is.vector(x)) {
stop("x should be a vector")
}
if (!is.vector(y)) {
stop("y should be a vector")
}
if (!is.vector(x0)) {
stop("x0 should be a vector")
}
# Check input values
if (length(x) != length(y)) {
stop("x and y should have a same length.")
}
if (length(unique(y)) < 2) {
stop("y should have at least 2 levels.")
}
# Create dataframe and remove missing values
df <- tibble(x, y) %>% drop_na()
# Check each level has at least 2 observations and non-zero variance
df_check <- df %>%
group_by(y) %>%
summarise(
count = n(),
variance = var(x),
.groups = 'drop'
)
if(any(df_check$count < 2)) {
stop("Each level of y should have at least 2 observations.")
}
if(any(df_check$variance == 0)) {
stop("Each level of y should have non-zero variance.")
}
# Get estimates
N <- length(x)
K <- length(unique(y))
# Remove missing values
df <- df %>% drop_na()
df <-
df %>%
group_by(y) %>%
mutate(
mu = mean(x),
pi = n() / N,
s2 = var(x)
)
# Extract the first row of each group for parameters
params <- df %>% distinct(y, mu, pi, s2)
mu <- params$mu
pi <- params$pi
s2 <- params$s2
class_labels <- params$y
# Calculate delta
delta <- matrix(numeric(length(x0) * K), ncol = K)
for(i in 1:K){
delta[, i] <- -0.5 * log(s2[i]) - 0.5 * ((x0 - mu[i])^2 / s2[i]) + log(pi[i])
}
# Determine the predicted class for each x0
max_delta_indices <- apply(delta, 1, which.max)
predicted_classes <- class_labels[max_delta_indices]
return(predicted_classes)
}
x <- 8:0
y <- rep(LETTERS[26:24], each = 3)
x0 <- c(7,5)
get_QDA(x = x, y = y, x0 = x0)
# 1. Basic case
x <- c(1, 2, 3, 4, 5, 6)
y <- c("A", "A", "A", "B", "B", "B")
x0 <- c(2.5, 4)
get_QDA(x = x, y = y, x0 = x0)
y <- c("A", "A", "A", "A", "A", "A")
x0 <- c(2.5, 4)
get_QDA(x = x, y = y, x0 = x0)
y <- c("A", "A", "A", "A", "A", "B")
x0 <- c(2.5, 4)
get_QDA(x = x, y = y, x0 = x0)
y <- c("A", "A", "A", "A", "B", "B")
x0 <- c(2.5, 4)
get_QDA(x = x, y = y, x0 = x0)
# 4. Zero variance
x <- c(1, 1, 3, 4, 5, 6)
y <- c("A", "A", "B", "B", "C", "C")
x0 <- c(2, 4)
get_QDA(x = x, y = y, x0 = x0)
# 4. Zero variance
x <- c(1, 2, 3, 4, 5, 6)
y <- c("A", "A", "B", "B", "C", "C")
x0 <- c(2, 4)
get_QDA(x = x, y = y, x0 = x0)
x <- 8:0
y <- rep(LETTERS[26:24], each = 3)
x0 <- c(7,5)
# Get estimates
N <- length(x)
K <- length(unique(y))
# Remove missing values
df <- df %>% drop_na()
# Remove missing values
df <- df %>% drop_na()
library(tidyverse)
library(dplyr)
# Remove missing values
df <- df %>% drop_na()
# Create dataframe and remove missing values
df <- tibble(x, y) %>% drop_na()
df <-
df %>%
group_by(y) %>%
mutate(
mu = mean(x),
pi = n() / N,
s2 = var(x)
)
df
# Extract the first row of each group for parameters
params <- df %>% distinct(y, mu, pi, s2)
params
mu <- params$mu
pi <- params$pi
s2 <- params$s2
mu
class_labels <- params$y
class_labels
# Calculate delta
delta <- matrix(numeric(length(x0) * K), ncol = K)
delta
for(i in 1:K){
delta[, i] <- -0.5 * log(s2[i]) - 0.5 * ((x0 - mu[i])^2 / s2[i]) + log(pi[i])
}
# Determine the predicted class for each x0
max_delta_indices <- apply(delta, 1, which.max)
max_delta_indices
predicted_classes <- class_labels[max_delta_indices]
predicted_classes
for(i in 1:K){
delta[, i] <- -0.5 * log(s2[i]) - 0.5 * ((x0 - mu[i])^2 / s2[i]) + log(pi[i])
}
delta
