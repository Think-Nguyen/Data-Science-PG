centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
centroids <- new_centroids
centroids
# Prepare data for plotting
data <- data.frame(index = 1:10000, value = x, cluster = factor(clusters))
# Plot the result
ggplot(data, aes(x = index, y = value, color = cluster)) +
geom_point(alpha = 0.6) +
labs(title = "k-means Clustering Result",
x = "Index",
y = "Value",
color = "Cluster") +
theme_minimal()
repeat {
# Calculate distances
distances <- cal_distance(x, centroids)
# Group distances
clusters <- max.col(-distances)
# Calculate new centroids
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
# Check for convergence
if (all(new_centroids == centroids)) break
centroids <- new_centroids
}
centroids
# Prepare data for plotting
data <- data.frame(index = 1:10000, value = x, cluster = factor(clusters))
# Plot the result
ggplot(data, aes(x = index, y = value, color = cluster)) +
geom_point(alpha = 0.6) +
labs(title = "k-means Clustering Result",
x = "Index",
y = "Value",
color = "Cluster") +
theme_minimal()
clusters
cluster_means <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
cluster_means
order_clusters <- order(cluster_means, decreasing = TRUE)
order_clusters
relabeled_clusters <- sapply(clusters, function(cluster) which(order_clusters == cluster))
relabeled_clusters
a <- c(2, 5, 8, 6, 7)
c <- c(3, 4, 5, 1, 2)
order_a <- order(a, decreasing = TRUE)
get_kmeans <- function(x, k) {
# Load necessary libraries
if (!require(tidyverse)) install.packages('tidyverse', dependencies=TRUE)
library(tidyverse)
if (!require(dplyr)) install.packages('dplyr', dependencies=TRUE)
library(dplyr)
# Check input types
if (!is.numeric(x)) {
stop('x should be a numeric vector.')
}
# Check input values
if (!is.numeric(k) || k < 1 || k > length(unique(x))) {
stop('k should be a numeric value between 1 and the number of unique values in x.')
}
# Set seed for reproducibility
set.seed(2024)
# Initialize centroids
centroids <- sample(unique(x), k)
# Create a function to calculate distance of centroids
cal_distance <- function(x, centroids) {
sapply(centroids, function(c) abs(x - c))
}
# Reassign centroids until stable
repeat {
# Calculate distances
distances <- cal_distance(x, centroids)
# Group distances
clusters <- max.col(-distances)
# Calculate new centroids
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
# Check for convergence
if (all(new_centroids == centroids)) break
centroids <- new_centroids
}
# Get order of centroids
order_centroids <- order(centroids, decreasing = TRUE)
# Relabel clusters from order centroids
relabeled_clusters <- sapply(clusters, function(cluster) which(order_centroids == cluster))
return(relabeled_clusters)
}
x <- rep(2:4, each = 3)
get_kmeans(x = x, 2.5)
get_kmeans(x = x, 3)
pacman::p_load(tidyverse, bookdown, readr, tidymodels, randomForest)
# Display the first 10 lines of the data
data %>% head(10)
# Read in the data
data <- readRDS('./diamonds2.rds')
pacman::p_load(tidyverse, bookdown, readr, tidymodels, randomForest)
# Read in the data
data <- readRDS('./diamonds2.rds')
# Display the first 10 lines of the data
data %>% head(10)
# Set the seed
set.seed(2024)
# Split data
data_split <- initial_split(data, strata = price)
data_split
data_train <- training(data_split)
data_train
data_test <- testing(data_split)
data_folds <- vfold_cv(data_train, v = 15, strata = price)
data_folds
data_recipe <- recipe(price ~ ., data = data_train) %>%
step_log(price) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
data_recipe
rf_model <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine('ranger') %>%
set_mode('regression')
wf <- workflow() %>%
add_recipe(data_recipe) %>%
add_model(rf_model)
wf
wf
# Set candidates
data_grid <- grid_regular(mtry(c(3,9)),
min_n(),
levels = 3)
# Tune
tune_res <- tune_grid(wf,
resamples = data_folds,
grid = data_grid)
tune_res
# Tune
tune_res <- tune_grid(wf,
resamples = data_folds,
grid = data_grid)
# Select the best hyperparameters
best_params <- select_best(tune_res, metric = 'rmse')
best_params
# Finalize the workflow with the best parameters
final_workflow <- finalize_workflow(wf, best_params)
final_workflow
# Fit the best model
final_fit <- last_fit(final_workflow, data_split)
# Evaluate the model on the test data
final_fit %>%
collect_metrics()
pacman::p_load(tidyverse, tidymodels, readr, skimr)
data <- readRDS('./sample_ex.rds')
pacman::p_load(tidyverse, tidymodels, readr, skimr)
data <- readRDS('./sample_ex.rds')
data
data %>% ggplot(aes(Y)) +
geom_histogram(bins = 20)
skim(data)
data_2 <- data %>%
mutate(Y = log(Y))
data_2 %>% summary()
data %>% summary()
data_2 %>% select(X2) %>% min()
outliers <- sort(data_2$X2)[1:5]
data_2$X2[data$X2 %in% outliers] <- NA
data_2 %>% summary()
data_2 <- data_2 %>%
mutate(X3 = as.numeric(X3))
data_2 %>% summary()
data_2 %>% count(C1)
c1_values <- c('a', 'b', 'c', 'd')
data_2$C1[!data_2$C1 %in% c1_values] <- NA
data_2 <- data_2 %>%
mutate(C1 = as.factor(C1))
data_2
data_2 %>% count(C2)
c2_values <- c('W', 'X', 'Y', 'Z')
data_2$C2[!data_2$C2 %in% c2_values] <- NA
data_2 <- data_2 %>%
mutate(C2 = as.factor(C2))
data_2
is.na(data_2) %>% sum()
data_2 <- data_2 %>% na.omit()
data_2
set.seed(20242)
data_split <- initial_split(data_2, strata = Y)
data_train <- training(data_split)
data_test <- testing(data_split)
data_folds <- vfold_cv(data_train, v = 20, strata = Y)
data_folds
data_recipe <- recipe(Y ~ ., data = data_train) %>%
step_BoxCox(X4) %>%
step_normalize(all_numeric_predictors()) %>%
step_interact(terms = ~ X1:X2) %>%
step_dummy(all_nominal_predictors())
data_recipe %>% prep() %>% bake(new_data = NULL)
pacman::p_load(glmnet)
las_reg_model <- linear_reg(mixture = 1, penalty = tune()) %>%
set_engine('glmnet')
wf <- workflow() %>%
add_recipe(data_recipe) %>%
add_model(las_reg_model)
penalty_grid <- grid_regular(penalty(), levels = 100)
penalty_grid$penalty[98]
doParallel::registerDoParallel()
tune_res <- tune_grid(wf,
resamples = data_folds,
grid = penalty_grid)
tune_res %>% autoplot()
show_best(tune_res, metric = 'rmse')
best_params <- select_best(tune_res, metric = 'rmse')
best_params
final_wf <- finalize_workflow(wf, best_params)
final_fit <- last_fit(final_wf, data_split)
final_fit %>% collect_metrics()
final_model <- final_fit %>% extract_workflow()
new_data <- tibble(
X1 = -1,
X2 = 0.5,
X3 = 2,
X4 = 1,
C1 = 'c',
C2 = 'X'
)
predict(final_model, new_data = new_data)
# Select the best hyperparameters
show_best(tune_res, metric = 'rmse')
best_params
final_workflow
# Finalize the workflow with the best parameters
final_workflow <- wf %>%
finalize_workflow(select_best(tune_res, metric = 'rmse'))
final_workflow
# Display the first 10 lines of the data
data %>% head(10)
dim(data_train)
data_train
data_split
# Read in the data
data <- readRDS('./diamonds2.rds')
data
# Split data
data_split <- initial_split(data, strata = price)
data_split
# Display the training/testing/total sets
data_split
data_split
data_folds <- vfold_cv(data_train, v = 15, strata = log(price))
data_train
# Read in the data
data <- readRDS('./diamonds2.rds')
# Display the first 10 lines of the data
data %>% head(10)
# Set the seed
set.seed(2024)
# Split data
data_split <- initial_split(data, strata = price)
data_train <- training(data_split)
data_test <- testing(data_split)
# Display the training/testing/total sets
data_split
data_folds <- vfold_cv(data_train, v = 15, strata = log(price))
data_train
data_folds <- vfold_cv(data_train, v = 15, strata = log(price))
data_folds
data_folds <- vfold_cv(data_train, v = 15, strata = price)
data_folds
# Set recipe
data_recipe <- recipe(price ~ ., data = data_train) %>%
step_log(price) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
# Display the recipe
data_recipe
# Set model
rf_model <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%
set_mode('regression')
# Set workflow
wf <- workflow() %>%
add_recipe(data_recipe) %>%
add_model(rf_model)
# Display the workflow
wf
grid_regular(mtry(c(3,9)),
min_n(),
levels = 3)
grid_regular(mtry(c(3,9)),
min_n(),
levels = 5)
tune_res
grid_regular(mtry(c(1,9)),
min_n(),
levels = 9)
grid_regular(mtry(c(1,2)),
min_n(),
levels = 9)
grid_regular(mtry(c(1,2)),
min_n(),
levels = 5)
grid_regular(mtry(c(1,2)),
min_n(),
levels = 2)
grid_regular(mtry(c(1,9)),
min_n(),
levels = 5)
# Plot tunning
tune_res %>% autoplot()
# Set candidates
data_grid <- grid_regular(mtry(c(1,9)),
min_n(),
levels = 5)
# Display the grid
data_grid
# Tune
doParallel::registerDoParallel()
tune_res <- tune_grid(wf,
resamples = data_folds,
grid = data_grid)
# Plot tunning
tune_res %>% autoplot()
# Select the best hyperparameters
show_best(tune_res, metric = 'rmse')
# Evaluate the model on the test data
final_fit %>%
collect_metrics()
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results')
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results',
x = 'The number of predictors')
knitr::opts_chunk$set(echo = TRUE,
message=FALSE,
warning = FALSE,
fig.align='center')
pacman::p_load(tidyverse, bookdown, readr, tidymodels)
# Read in the data
data <- readRDS('./diamonds2.rds')
# Display the first 10 lines of the data
data %>% head(10)
# Set the seed
set.seed(2024)
# Split data
data_split <- initial_split(data, strata = price)
data_train <- training(data_split)
data_test <- testing(data_split)
# Display the training/testing/total sets
data_split
# Set folds cross-validation
data_folds <- vfold_cv(data_train, v = 15, strata = price)
# Display the folds
data_folds
# Set recipe
data_recipe <- recipe(price ~ ., data = data_train) %>%
step_log(price)
# Display the recipe
data_recipe
# Set model
rf_model <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%
set_mode('regression')
# Set workflow
wf <- workflow() %>%
add_recipe(data_recipe) %>%
add_model(rf_model)
# Display the workflow
wf
# Set candidates
data_grid <- grid_regular(mtry(c(1,9)),
min_n(),
levels = 5)
# Display the grid
data_grid
# Tune
doParallel::registerDoParallel()
tune_res <- tune_grid(wf,
resamples = data_folds,
grid = data_grid)
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results',
x = 'Number of predictors')
# Show the top 5 best hyperparameters
show_best(tune_res, metric = 'rmse')
# Finalize the workflow with the best parameters
final_workflow <- wf %>%
finalize_workflow(select_best(tune_res, metric = 'rmse'))
# Display the final workflow
final_workflow
# Fit the best model
final_fit <- last_fit(final_workflow, data_split)
# Evaluate the model on the test data
final_fit %>%
collect_metrics()
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results',
x = 'Number of predictors') +
theme(plot.title = element_text(size = 20))
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results',
x = 'Number of predictors') +
theme(plot.title = element_text(size = 10))
# Plot tuning
tune_res %>% autoplot() +
labs(title = 'Tuning results',
x = 'Number of predictors')
x <- sample(1:10000)
k <- 4
# Set seed for reproducibility
set.seed(2024)
# Initialize centroids
centroids <- sample(unique(x), k)
centroids
# Create a function to calculate distance of centroids
cal_distance <- function(x, centroids) {
sapply(centroids, function(c) abs(x - c))
}
distances <- cal_distance(x, centroids)
clusters <- max.col(-distances)
#clusters %>% table()
# Prepare data for plotting
data <- data.frame(index = 1:10000, value = x, cluster = factor(clusters))
# Plot the result
ggplot(data, aes(x = index, y = value, color = cluster)) +
geom_point(alpha = 0.6) +
labs(title = "k-means Clustering Result",
x = "Index",
y = "Value",
color = "Cluster") +
theme_minimal()
repeat {
# Calculate distances
distances <- cal_distance(x, centroids)
# Group distances
clusters <- max.col(-distances)
# Calculate new centroids
new_centroids <- sapply(1:k, function(i) mean(x[clusters == i], na.rm = TRUE))
# Check for convergence
if (all(new_centroids == centroids)) break
centroids <- new_centroids
}
# Plot the result
ggplot(data, aes(x = index, y = value, color = cluster)) +
geom_point(alpha = 0.6) +
labs(title = "k-means Clustering Result",
x = "Index",
y = "Value",
color = "Cluster") +
theme_minimal()
centroids
